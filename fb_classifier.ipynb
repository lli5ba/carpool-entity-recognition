{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import nltk\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv('posts/shuffled_posts.csv', encoding = 'latin1')\n",
    "relevant_ix = [] # non-spam indices\n",
    "all_ix = [] # includes spam\n",
    "for index, row in data.iterrows():\n",
    "    if row['is_relevant'] == 1:\n",
    "        relevant_ix.append(index)\n",
    "        all_ix.append(index)\n",
    "    if row['is_relevant'] == 0:\n",
    "        all_ix.append(index)\n",
    "data_annotated = data.ix[relevant_ix]\n",
    "data_all = data.ix[all_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "text = []\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "for index, row in data_annotated.iterrows():\n",
    "    full_text = row['status_message'].lower()\n",
    "    tk_text = nltk.word_tokenize(full_text)\n",
    "    stemmed = [stemmer.stem(token) for token in tk_text]\n",
    "    text.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct vocabulary with ttf\n",
    "k = 10\n",
    "ttf = {}\n",
    "for words in text:\n",
    "    for word in words:\n",
    "        if word not in ttf:\n",
    "            ttf[word] = 1\n",
    "        else:\n",
    "            ttf[word] += 1\n",
    "vocab1 = [word for word in ttf.keys() if ttf[word] >= k and not re.search('\\d', word)] # high word count and not date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create feature vectors\n",
    "X = []\n",
    "for row in text:\n",
    "    X_row = []\n",
    "    for voc in vocab1:\n",
    "        if voc in row:\n",
    "            X_row.append(1)\n",
    "        else:\n",
    "            X_row.append(0)\n",
    "    X.append(X_row)\n",
    "X = np.matrix(X)\n",
    "y_driver = np.array(data_annotated['is_driver'])\n",
    "y_roundtrip = np.array(data_annotated['is_roundtrip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# machine learning, driver or rider\n",
    "clfs = [MultinomialNB(), BernoulliNB(), LogisticRegression(), LinearSVC(C=.1), DecisionTreeClassifier('entropy'), \n",
    "        RandomForestClassifier(), KNeighborsClassifier(n_neighbors=3)]\n",
    "for clf in clfs:\n",
    "    scores = cross_val_score(clf, X, y_driver, cv = 5)\n",
    "    print(clf)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# best classifier\n",
    "clf_driver = MultinomialNB()\n",
    "clf_driver.fit(X, y_driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# machine learning, round trip?\n",
    "clfs = [MultinomialNB(), BernoulliNB(), LogisticRegression(C=.1), LinearSVC(C=.01), DecisionTreeClassifier('entropy'), \n",
    "        RandomForestClassifier(), KNeighborsClassifier(n_neighbors=3)]\n",
    "for clf in clfs:\n",
    "    scores = cross_val_score(clf, X, y_roundtrip, cv = 5)\n",
    "    print(clf)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing, spam\n",
    "text = []\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "for index, row in data_all.iterrows():\n",
    "    full_text = row['status_message'].lower()\n",
    "    tk_text = nltk.word_tokenize(full_text)\n",
    "    stemmed = [stemmer.stem(token) for token in tk_text]\n",
    "    text.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_roundtrip = LinearSVC(C=.01)\n",
    "clf_roundtrip.fit(X, y_roundtrip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct vocabulary with ttf, spam\n",
    "k = 10\n",
    "ttf = {}\n",
    "for words in text:\n",
    "    for word in words:\n",
    "        if word not in ttf:\n",
    "            ttf[word] = 1\n",
    "        else:\n",
    "            ttf[word] += 1\n",
    "vocab2 = [word for word in ttf.keys() if ttf[word] >= k and not re.search('\\d', word)] # high word count and not date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create feature vectors, spam\n",
    "X = []\n",
    "for row in text:\n",
    "    X_row = []\n",
    "    for voc in vocab2:\n",
    "        if voc in row:\n",
    "            X_row.append(1)\n",
    "        else:\n",
    "            X_row.append(0)\n",
    "    X.append(X_row)\n",
    "X = np.matrix(X)\n",
    "y_relevant = np.array(data_all['is_relevant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# machine learning, spam\n",
    "clfs = [MultinomialNB(), BernoulliNB(), LogisticRegression(), LinearSVC(), DecisionTreeClassifier('entropy'), \n",
    "        RandomForestClassifier(criterion='entropy'), KNeighborsClassifier(n_neighbors=1)]\n",
    "for clf in clfs:\n",
    "    scores = cross_val_score(clf, X, y_relevant, cv = 5)\n",
    "    print(clf)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_relevant = LogisticRegression()\n",
    "clf_relevant.fit(X, y_relevant)\n",
    "y_pred = clf_relevant.predict(X)\n",
    "# accuracy metrics when applied on training set\n",
    "precision_recall_fscore_support(y_relevant, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use these classifiers on rest of dataset\n",
    "count = 0\n",
    "for index, row in data.iterrows():\n",
    "    full_text = row['status_message'].lower()\n",
    "    tk_text = nltk.word_tokenize(full_text)\n",
    "    stemmed = [stemmer.stem(token) for token in tk_text]\n",
    "    vec1 = np.array([[int(voc in stemmed) for voc in vocab1]])\n",
    "    vec2 = np.array([[int(voc in stemmed) for voc in vocab2]])\n",
    "    # not annotated row\n",
    "    if index not in all_ix:\n",
    "        count += 1\n",
    "        if not count % 1000:\n",
    "            print(count)\n",
    "        data.ix[index, 'is_driver'] = clf_driver.predict(vec1)\n",
    "        data.ix[index, 'is_roundtrip'] = clf_roundtrip.predict(vec1)\n",
    "        data.ix[index, 'is_relevant'] = clf_relevant.predict(vec2)\n",
    "# roughly 2000 spam entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.to_csv('posts/spam_annotated_posts.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
