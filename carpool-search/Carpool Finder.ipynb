{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from django.http import HttpResponse, JsonResponse\n",
    "import json, os, re, urllib2, datetime\n",
    "import pickle\n",
    "from nltk import StanfordNERTagger\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sutime import SUTime\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import imp\n",
    "import jpype\n",
    "import socket\n",
    "import threading\n",
    "import json\n",
    "import time\n",
    "import gmaps\n",
    "import gmaps.datasets  \n",
    "debug = False\n",
    "showcase = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#location global vars\n",
    "stanford_dir = os.path.join(os.path.dirname(os.path.realpath('__file__')), 'stanfordjars')\n",
    "st = StanfordNERTagger(os.path.join(stanford_dir, 'ner-model.ser.gz'), os.path.join(stanford_dir, 'stanford-ner.jar'))\n",
    "st._stanford_jar = os.path.join(stanford_dir, '*')\n",
    "place_to_coords = {}\n",
    "url_base = 'https://maps.googleapis.com/maps/api/place/textsearch/json'\n",
    "api_key = 'AIzaSyAVat82-OUFKC9GpyOi3LNyQKwxE2KWY9U'\n",
    "\n",
    "#time global vars\n",
    "jar_files = os.path.join(os.path.dirname(os.path.realpath('__file__')), 'sutimejars')\n",
    "sutime = SUTime(jars=jar_files, mark_time_ranges=True)\n",
    "\n",
    "#FB api global vars\n",
    "app_id = \"1696549057338916\"\n",
    "app_secret = \"21090405ac37194a1d4578aeb2371845\" # DO NOT SHARE WITH ANYONE!\n",
    "access_token = app_id + \"|\" + app_secret\n",
    "\n",
    "#classifier global vars\n",
    "def unpickle():\n",
    "    pickle_dir = os.path.join(os.path.dirname(os.path.realpath('__file__')), 'pickles')\n",
    "    with open(os.path.join(pickle_dir, 'clf_driver.pkl'), 'rb') as fid:\n",
    "        clf_driver = pickle.load(fid)\n",
    "    with open(os.path.join(pickle_dir, 'clf_roundtrip.pkl'), 'rb') as fid:\n",
    "        clf_roundtrip = pickle.load(fid)\n",
    "    with open(os.path.join(pickle_dir, 'clf_relevant.pkl'), 'rb') as fid:\n",
    "        clf_relevant = pickle.load(fid)\n",
    "    with open(os.path.join(pickle_dir, 'vocab1.pkl'), 'rb') as fid:\n",
    "        vocab1 = pickle.load(fid)\n",
    "    with open(os.path.join(pickle_dir, 'vocab2.pkl'), 'rb') as fid:\n",
    "        vocab2 = pickle.load(fid)\n",
    "    return clf_driver, clf_roundtrip, clf_relevant, vocab1, vocab2\n",
    "\n",
    "clf_driver, clf_roundtrip, clf_relevant, vocab1, vocab2 = unpickle()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27151997,  0.46785491,  0.39798693,  0.20443933,  0.11506808,\n",
       "         0.59105383, -0.26264155,  0.32502873,  0.2922863 ,  0.73504264,\n",
       "         0.13988957,  0.23876058, -0.54548802, -0.23283322,  0.36713866,\n",
       "         0.12310677,  0.53730076,  1.4167365 ,  0.69078322,  0.35451261,\n",
       "         0.03494293,  0.13374672, -0.82424347,  0.60896177,  0.02917146,\n",
       "         0.11818612, -0.74739414,  0.35020363,  0.5490217 ,  0.08705915,\n",
       "        -0.33452811,  0.08815023,  0.11418465,  0.08229926,  0.24577413,\n",
       "         0.63989104, -0.2046203 ,  0.15308856, -0.17904911, -0.44806462,\n",
       "         0.13201955, -0.26309126, -0.15307325,  0.03994454,  0.35409521,\n",
       "         0.91598162,  0.38377528,  0.10981995,  0.22114511,  0.23807354,\n",
       "         0.3592669 ,  0.94092647, -0.29816948,  0.16152207,  0.25129427,\n",
       "        -0.18292593, -0.2734331 ,  0.19026512,  0.12722003, -0.17448921,\n",
       "         0.3201431 ,  0.05183653,  0.44757136, -0.32015262,  0.09960963,\n",
       "         0.26841686,  0.31378553,  0.025068  ,  0.47977558, -0.37482802,\n",
       "         0.07401369, -0.08604502,  0.14276834, -0.84127062,  0.03295445,\n",
       "        -0.36774606, -0.06232048,  0.51020794,  0.09722205,  0.40052668,\n",
       "         0.49611312,  0.341566  ,  0.04998902, -0.79910513,  0.09532982,\n",
       "        -0.24026031,  0.27008544, -0.47257306,  0.07891119,  0.11063625,\n",
       "        -0.15756961,  0.11472851,  0.59381768,  0.24742471, -0.78244697]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clf_driver.feature_importances_\n",
    "#clf_roundtrip.feature_importances_\n",
    "clf_relevant.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"9c449842-08c3-4fdf-8a65-9637df0c4aee\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = \"1\";\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      Bokeh.$(\"#9c449842-08c3-4fdf-8a65-9637df0c4aee\").text(\"BokehJS successfully loaded.\");\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"9c449842-08c3-4fdf-8a65-9637df0c4aee\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '9c449842-08c3-4fdf-8a65-9637df0c4aee' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      Bokeh.$(\"#9c449842-08c3-4fdf-8a65-9637df0c4aee\").text(\"BokehJS is loading...\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === \"1\") {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (!force) {\n",
       "      var cell = $(\"#9c449842-08c3-4fdf-8a65-9637df0c4aee\").parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gmaps\n",
    "from bokeh.plotting import output_notebook\n",
    "gmaps.configure(api_key=api_key) # Your Google API key\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_results(request):\n",
    "    if request.method != 'GET':\n",
    "        return _error_response(request, \"Should be GET request\")\n",
    "    if 'group_id' in request.GET and 'home_location' in request.GET \\\n",
    "            and 'state' in request.GET and 'num_posts' in request.GET:\n",
    "        group_id = request.GET['group_id']\n",
    "        home_loc_string = request.GET['home_location']\n",
    "        state = request.GET['state']\n",
    "        num_posts = request.GET['num_posts']\n",
    "    else:\n",
    "        return _error_response(request, \"Missing url params\")\n",
    "    results = process_fb_group(group_id, home_loc_string, state, num_posts)\n",
    "    return _success_response(request, results)\n",
    "\n",
    "def _error_response(request, error_msg):\n",
    "    return JsonResponse({'ok': False, 'error': error_msg})\n",
    "\n",
    "def _success_response(request, resp=None):\n",
    "    if resp:\n",
    "        return JsonResponse({'ok': True, 'resp': resp})\n",
    "    else:\n",
    "        return JsonResponse({'ok': True})\n",
    "\n",
    "\n",
    "\n",
    "def load_data(csv_fpath):\n",
    "        print(\"loading data\")\n",
    "        data = pd.read_csv(csv_fpath, encoding = 'latin1')\n",
    "        return data\n",
    "\n",
    "def request_until_succeed(url):\n",
    "    req = urllib2.Request(url)\n",
    "    success = False\n",
    "    while success is False:\n",
    "        try:\n",
    "            response = urllib2.urlopen(req)\n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception, e:\n",
    "            print e\n",
    "            datetime.time.sleep(5)\n",
    "\n",
    "            print \"Error for URL %s: %s\" % (url, datetime.datetime.now())\n",
    "            print \"Retrying.\"\n",
    "\n",
    "    return response.read()\n",
    "\n",
    "# Needed for tricky unicode processing\n",
    "def unicode_normalize(text):\n",
    "    return text.translate({ 0x2018:0x27, 0x2019:0x27, 0x201C:0x22, 0x201D:0x22,\n",
    "                            0xa0:0x20 }).encode('utf-8')\n",
    "\n",
    "def process_fb_group(group_id, home_loc_string, state, k):\n",
    "    home_coord = get_home_location(home_loc_string, state)\n",
    "    #setup json dictionary\n",
    "    results = {}\n",
    "    dates = {}\n",
    "    locations = {}\n",
    "    posts = {}\n",
    "    # 2nd level\n",
    "    dates_drivers = defaultdict(list)\n",
    "    #will create a new list object, if the key is not found in the dictionary\n",
    "    dates_riders = defaultdict(list)\n",
    "    locations_drivers = defaultdict(list)\n",
    "    locations_riders = defaultdict(list)\n",
    "    posts_drivers = {}\n",
    "    posts_riders = {}\n",
    "\n",
    "    has_next_page = True\n",
    "    num_processed = 0\n",
    "\n",
    "    statuses = getFacebookPageFeedData(group_id, access_token, 100)\n",
    "    while has_next_page:\n",
    "        for status in statuses['data']:\n",
    "            if num_processed >= k:\n",
    "                break\n",
    "            # Ensure it is a status with the expected metadata\n",
    "            if 'message' in status:\n",
    "                status_id, status_permalinkurl, status_published,\\\n",
    "                status_message, status_location, status_price,\\\n",
    "                status_author, status_type, status_link,\\\n",
    "                link_name = processFacebookPageFeedStatus(status,access_token)\n",
    "                if debug:\n",
    "                    print(status_message)\n",
    "                is_driver, is_roundtrip, is_relevant = predict(status_message)\n",
    "                if is_relevant: #not spam so continue processing\n",
    "                    coordinates = process_sentence(status_message, home_coord, state)\n",
    "                    if debug:\n",
    "                        print(coordinates)\n",
    "                    date_list = get_dates(status_message, status_published)\n",
    "                    if debug:\n",
    "                        print(date_list)\n",
    "                    routes = []\n",
    "                    if len(coordinates) == 1:\n",
    "                        routes.append({'start':coordinates[0],\n",
    "                                      'end':coordinates[0]})\n",
    "                    elif len(coordinates) == 2:\n",
    "                        routes.append({'start':coordinates[0],\n",
    "                                      'end':coordinates[1]})\n",
    "                        if len(coordinates) == 4:\n",
    "                            routes.append({'start':coordinates[2],\n",
    "                                      'end':coordinates[3]})\n",
    "                    #print(routes)\n",
    "                    if is_driver: #add to drivers\n",
    "                        for date in date_list:\n",
    "                            dates_drivers[date].append(status_permalinkurl)\n",
    "                        for coord in coordinates:\n",
    "                            latlng = (coord['lat'], coord['lng'])\n",
    "                            locations_drivers[latlng].append(status_permalinkurl)\n",
    "                        posts_drivers[status_permalinkurl] = {'routes': routes}\n",
    "                    else: #add to riders\n",
    "                        for date in date_list:\n",
    "                            dates_riders[date].append(status_permalinkurl)\n",
    "                        for coord in coordinates:\n",
    "                            latlng = (coord['lat'], coord['lng'])\n",
    "                            locations_riders[latlng].append(status_permalinkurl)\n",
    "                        posts_riders[status_permalinkurl] = {'routes': routes}\n",
    "\n",
    "            num_processed += 1\n",
    "\n",
    "        # if there is no next page, we're done.\n",
    "        if 'paging' in statuses.keys() and num_processed < k:\n",
    "            statuses = json.loads(request_until_succeed(\\\n",
    "                    statuses['paging']['next']))\n",
    "        else:\n",
    "            has_next_page = False\n",
    "\n",
    "    # 2nd level\n",
    "    dates['drivers'] = dict(dates_drivers)\n",
    "    dates['riders'] = dict(dates_riders)\n",
    "    locations['drivers'] = remap_keys(locations_drivers)\n",
    "    locations['riders'] = remap_keys(locations_riders)\n",
    "    posts['drivers'] = posts_drivers\n",
    "    posts['riders'] = posts_riders\n",
    "\n",
    "    #1st level\n",
    "    results['dates'] = dates\n",
    "    results['locations'] = locations\n",
    "    results['posts'] = posts\n",
    "\n",
    "    #print(results)\n",
    "    return results\n",
    "\n",
    "def remap_keys(mapping):\n",
    "    return [{'key':k, 'value': v} for k, v in mapping.iteritems()]\n",
    "\n",
    "def process_custom_post(status_message, home_location, state, status_permalinkurl = ''):\n",
    "    home_coord = get_coords([home_location], state)\n",
    "    is_driver, is_roundtrip, is_relevant = predict(status_message)\n",
    "    if is_relevant: #not spam so continue processing\n",
    "        coordinates, locations = process_sentence2(status_message, home_coord, home_location, state)\n",
    "        if debug:\n",
    "            print(coordinates)\n",
    "        status_published = time.strftime('%m/%d/%Y %H:%M') \n",
    "        date_list = get_dates(status_message, status_published)\n",
    "        if debug:\n",
    "            print(date_list)\n",
    "        routes = []\n",
    "        lats = []\n",
    "        lngs = []\n",
    "        for coord in coordinates:\n",
    "            lats.append(coord['lat'])\n",
    "            lngs.append(coord['lng'])\n",
    "        if len(coordinates) == 1:\n",
    "            if home_coord:\n",
    "                if 'lat' in home_coord[0] and 'lng' in home_coord[0]:\n",
    "                    lats.append(home_coord[0]['lat'])\n",
    "                    lngs.append(home_coord[0]['lng'])\n",
    "            routes.append({'start':coordinates[0],\n",
    "                          'end':coordinates[0]})\n",
    "        elif len(coordinates) == 2:\n",
    "            routes.append({'start':coordinates[0],\n",
    "                          'end':coordinates[1]})\n",
    "            if len(coordinates) == 4:\n",
    "                routes.append({'start':coordinates[2],\n",
    "                          'end':coordinates[3]})\n",
    "        return lats, lngs, date_list, status_message, status_permalinkurl, home_coord, locations\n",
    "            \n",
    "            \n",
    "def processFacebookPageFeedStatus(status, access_token):\n",
    "\n",
    "    # The status is now a Python dictionary, so for top-level items,\n",
    "    # we can simply call the key.\n",
    "\n",
    "    # Additionally, some items may not always exist,\n",
    "    # so must check for existence first\n",
    "\n",
    "    if 'message' in status.keys():\n",
    "        #print unicode_normalize(status['message'])\n",
    "        #split on \\n and remove empty lines\n",
    "        lines = filter(None, [s.strip() for s in status['message'].splitlines()])\n",
    "        #handle Sale Posting which contain either a price and location, or just a price\n",
    "        #It is a sale posting if the 2nd line of the message starts with a price or 'FREE'\n",
    "        #Split the line to handle the case where someone started a line in their message\n",
    "        # with a price, but it is not actually a sale post\n",
    "        if len(lines) > 1 and (re.match('(^\\$\\d+)',\n",
    "                                        [s.strip() for s in lines[1].split('-')][0])\n",
    "                               or [s.strip() for s in lines[1].split('-')][0] == 'FREE'):\n",
    "            #split on '-' and strip whitespace to separate out location, if there is one\n",
    "            line_split = [s.strip() for s in lines[1].split('-')]\n",
    "            status_price = line_split[0]\n",
    "            status_location = '' if len(line_split) < 2 \\\n",
    "                else line_split[1]\n",
    "            # set message equal to title\n",
    "            status_message =  lines[0]\n",
    "            # if title did not end in punctuation, append a period\n",
    "            if lines[0][-1] not in '!?.,':\n",
    "                status_message += '.'\n",
    "            # append remaining description in sale post if they exists\n",
    "            if len(lines) > 2:\n",
    "                status_message += ' ' + ' '.join(lines[2:])\n",
    "        else: #not a sale post\n",
    "            status_location = ''\n",
    "            status_price = ''\n",
    "            status_message = ' '.join(lines)\n",
    "        status_message = unicode_normalize(status_message)\n",
    "    else:\n",
    "        status_message = ''\n",
    "        status_location = ''\n",
    "        status_price = ''\n",
    "\n",
    "    status_id = '' if 'id' not in status.keys() else \\\n",
    "        status['id']\n",
    "    link_name = '' if 'name' not in status.keys() else \\\n",
    "            unicode_normalize(status['name'])\n",
    "    status_type = '' if 'type' not in status.keys() else \\\n",
    "            status['type']\n",
    "    status_link = '' if 'link' not in status.keys() else \\\n",
    "            unicode_normalize(status['link'])\n",
    "    status_author = '' if 'from' not in status.keys() else \\\n",
    "        unicode_normalize(status['from']['name'])\n",
    "    status_permalinkurl = '' if 'permalink_url' not in status.keys() else\\\n",
    "        status['permalink_url']\n",
    "\n",
    "    # Time needs special care since a) it's in UTC and\n",
    "    # b) it's not easy to use in statistical programs.\n",
    "    if 'created_time' in status.keys():\n",
    "        status_published = datetime.datetime.strptime(\\\n",
    "                status['created_time'],'%Y-%m-%dT%H:%M:%S+0000')\n",
    "        status_published = status_published + datetime.timedelta(hours=-5) # EST\n",
    "        # best time format for spreadsheet programs:\n",
    "        status_published = status_published.strftime('%m/%d/%Y %H:%M')\n",
    "    else:\n",
    "        status_published = ''\n",
    "\n",
    "\n",
    "    # return a tuple of all processed data\n",
    "\n",
    "    return (status_id, status_permalinkurl, status_published,\n",
    "            status_message, status_location, status_price,\n",
    "            status_author, status_type, status_link,\n",
    "            link_name)\n",
    "\n",
    "\n",
    "def get_home_location(home, state):\n",
    "    coordinates = get_coords([home], state)\n",
    "    if coordinates:\n",
    "        return coordinates[0]\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def process_sentence(message, home_coord, state):\n",
    "    tokenized = nltk.word_tokenize(message)\n",
    "    tagged_sent = st.tag(tokenized)\n",
    "    locations = extract_chunks(tagged_sent)\n",
    "    coordinates = get_coords(locations, state)\n",
    "    if debug:\n",
    "        print(\"length of coordinates\" + str(len(coordinates)))\n",
    "    if len(coordinates) == 1:\n",
    "        #add home location to location list\n",
    "        if 'lat' in home_coord and 'lng' in home_coord:\n",
    "            coordinates.append(home_coord)\n",
    "        return coordinates\n",
    "    elif len(coordinates) == 3:\n",
    "        return coordinates[:2]\n",
    "    elif len(coordinates) > 4:\n",
    "        return coordinates[:4]\n",
    "    else:\n",
    "        return coordinates\n",
    "\n",
    "def process_sentence2(message, home_coord, home, state):\n",
    "    tokenized = nltk.word_tokenize(message)\n",
    "    tagged_sent = st.tag(tokenized)\n",
    "    locations = extract_chunks(tagged_sent)\n",
    "    coordinates = get_coords(locations, state)\n",
    "    if debug:\n",
    "        print(\"length of coordinates\" + str(len(coordinates)))\n",
    "    if len(coordinates) == 1:\n",
    "        #add home location to location list\n",
    "        locations.append(home)\n",
    "        if 'lat' in home_coord and 'lng' in home_coord:\n",
    "            coordinates.append(home_coord)\n",
    "        return coordinates, locations\n",
    "    elif len(coordinates) == 3:\n",
    "        return coordinates[:2], locations\n",
    "    elif len(coordinates) > 4:\n",
    "        return coordinates[:4], locations\n",
    "    else:\n",
    "        return coordinates, locations\n",
    "\n",
    "\n",
    "def getFacebookPageFeedData(group_id, access_token, num_statuses):\n",
    "\n",
    "    # Construct the URL string; see\n",
    "    # http://stackoverflow.com/a/37239851 for Reactions parameters\n",
    "    base = \"https://graph.facebook.com/v2.8\"\n",
    "    node = \"/%s/feed\" % group_id\n",
    "    fields = \"/?fields=message,link,created_time,type,name,id,permalink_url,attachments,\" + \\\n",
    "            \"comments.limit(0).summary(true),shares,reactions.\" + \\\n",
    "            \"limit(0).summary(true),from\"\n",
    "    parameters = \"&limit=%s&access_token=%s\" % (num_statuses, access_token)\n",
    "    url = base + node + fields + parameters\n",
    "\n",
    "    # retrieve data\n",
    "    data = json.loads(request_until_succeed(url))\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_all(messages, home_coord, state):\n",
    "    for (message, _) in messages:\n",
    "        process_sentence(message, home_coord, state)\n",
    "\n",
    "def extract_chunks(tagged_sent, chunk_type='LOC'):\n",
    "    locations = []\n",
    "    chain = False\n",
    "    location = ''\n",
    "    for (word, tag) in tagged_sent:\n",
    "        if tag == chunk_type and not chain: #start of chain\n",
    "            location += word\n",
    "            chain = True\n",
    "        elif tag == chunk_type and chain: #add on to chain\n",
    "            location += \" \" + word\n",
    "        elif tag != chunk_type and chain: #chain ended\n",
    "            locations.append(location)\n",
    "            location = ''\n",
    "            chain = False\n",
    "    return locations\n",
    "\n",
    "def remove_dups(locations):\n",
    "    locs = set()\n",
    "    result = []\n",
    "    for item in locations:\n",
    "        if item not in locs:\n",
    "            locs.add(item)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_coords(locations, state):\n",
    "\n",
    "    global place_to_coords\n",
    "    coordinates = []\n",
    "    locations = remove_dups(locations)\n",
    "    for location in locations:\n",
    "        #construct uri\n",
    "        if state != '':\n",
    "            location += \" \" + state\n",
    "        location = location.lower()\n",
    "        if location not in ['back', 'campus']:\n",
    "            location = location.replace('757', 'virginia beach')\n",
    "            if location in place_to_coords:\n",
    "                coord = place_to_coords[location]\n",
    "                coordinates.append(coord)\n",
    "            else:\n",
    "                parameters = {'query': location, 'key': api_key}\n",
    "                r = requests.get(url_base, params=parameters)\n",
    "                if r.status_code == 200:\n",
    "                    response = r.json()\n",
    "                    if 'results' in response:\n",
    "                        results = response['results']\n",
    "                        if results and 'geometry' in results[0]:\n",
    "                            #get coordinates\n",
    "                            if debug:\n",
    "                                print results[0]\n",
    "                            geometry = results[0]['geometry']\n",
    "                            if 'location' in geometry:\n",
    "                                coord = geometry['location']\n",
    "                                coordinates.append(coord)\n",
    "                                place_to_coords[location] = coord\n",
    "\n",
    "    #print(coordinates)\n",
    "    return coordinates\n",
    "\n",
    "def get_dates(message, date):\n",
    "    dates = []\n",
    "    date_posted = datetime.datetime.strptime(date, '%m/%d/%Y %H:%M')\n",
    "    if debug:\n",
    "        print(date_posted)\n",
    "    try:\n",
    "        if debug:\n",
    "            print(sutime._required_jars)\n",
    "        sutime.threadfix()\n",
    "        times = sutime.parsedate(message, date)\n",
    "        print(times)\n",
    "        if debug:\n",
    "            print(times)\n",
    "        for time in times:\n",
    "            val = time['value']\n",
    "            if 'WE' in val:\n",
    "                #friday, sat, sunday\n",
    "                candidate_dates = [datetime.datetime.strptime(val[:8] + \"-5\", \"%Y-W%W-%w\"),\n",
    "                                   datetime.datetime.strptime(val[:8] + \"-6\", \"%Y-W%W-%w\"),\n",
    "                                   datetime.datetime.strptime(val[:8] + \"-0\", \"%Y-W%W-%w\")]\n",
    "                for candidate in candidate_dates:\n",
    "                    if candidate >= date_posted:\n",
    "                        dates.append(candidate)\n",
    "            else:\n",
    "                candidate = datetime.datetime.strptime(val[:10], \"%Y-%m-%d\")\n",
    "                if candidate >= date_posted:\n",
    "                        dates.append(candidate)\n",
    "\n",
    "    except Exception, e:\n",
    "        print str(e)\n",
    "    if not dates:\n",
    "            dates.append(date_posted)\n",
    "    dates_formatted = [date_obj.strftime('%m/%d/%Y') for date_obj in dates]\n",
    "    return dates_formatted\n",
    "\n",
    "\n",
    "def feature_vector(msg, vocab):\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    msg = msg.lower()\n",
    "    tk_text = nltk.word_tokenize(msg)\n",
    "    stemmed = [stemmer.stem(token) for token in tk_text]\n",
    "    vec = np.array([[int(voc in stemmed) for voc in vocab]])\n",
    "    return vec\n",
    "\n",
    "def predict(msg):\n",
    "    vec1 = feature_vector(msg, vocab1)\n",
    "    vec2 = feature_vector(msg, vocab2)\n",
    "    is_driver = clf_driver.predict(vec1)\n",
    "    is_roundtrip = clf_roundtrip.predict(vec1)\n",
    "    is_relevant = clf_relevant.predict(vec2)\n",
    "    return is_driver, is_roundtrip, is_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'start': 19, u'type': u'TIME', u'end': 39, u'value': u'2016-W49-WET10:00', u'text': u'next weekend at 10am'}]\n",
      "[u'UMD', 'duke']\n",
      "['12/09/2016', '12/10/2016', '12/11/2016']\n",
      "[35.993116, 36.0014258]\n",
      "[-78.895358, -78.9382286]\n"
     ]
    }
   ],
   "source": [
    "home_location = \"duke\"\n",
    "state = \"N Carolina\"\n",
    "message = \"Looking for a ride next weekend at 10am to UMD! I can pitch in for gas money. Thanks!\"\n",
    "lats, lngs, date_list, status_message, status_permalinkurl, home_coord, locations = process_custom_post(message, home_location, state)\n",
    "print(locations)\n",
    "print(date_list)\n",
    "print(lats)\n",
    "print(lngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "marker_locations = zip(lats, lngs)\n",
    "markers = gmaps.marker_layer(marker_locations)\n",
    "\n",
    "m = gmaps.Map()\n",
    "m.add_layer(markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (marker_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
